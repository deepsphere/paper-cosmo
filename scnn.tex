%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

% \documentclass[preprint,12pt,authoryear]{elsarticle}
\documentclass[final,twocolumn,3p,times,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}


%\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{color}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx,array}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{aasmacros}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\newcommand{\nati}[1]{{\color[rgb]{.1,.6,.1}{#1}}}
\newcommand{\todo}[1]{{\color[rgb]{.6,.1,.6}{#1}}}
\newcommand{\assign}[1]{{\color[rgb]{.8,.5,.8}{Assigned: #1 }}}

\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
%\newcommand{\secref}[1]{\S\ref{sec:#1}}
\newcommand{\eqnref}[1]{(\ref{eqn:#1})}

\renewcommand{\b}[1]{{\bm{#1}}}   % bold symbol

% MATH SYMBOLS
\newcommand{\1}{\b{1}}              % all-ones vector
\newcommand{\0}{\b{0}}              % all-zero vector
\newcommand{\g}[1]{\b{#1}}
\renewcommand{\L}{\b{L}} % the laplacian matrix
\newcommand{\W}{\b{W}}
\newcommand{\I}{\b{I}}
\newcommand{\D}{\b{D}}
\newcommand{\U}{\b{U}}
\newcommand{\bLambda}{\b{\Lambda}}
\newcommand{\blambda}{\b{\lambda}}
\newcommand{\pkg}[1]{\texttt{#1}}


\journal{Astronomy and Computing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{HealpixNet: Efficient spherical Convolutional Neural Network with Healpix sampling for cosmological applications}
% \title{Efficient Spherical ConvNet with Graph Convolutions for Cosmological Applications}
% \title{HealpixNet: Efficient spherical Convolutional Neural Network with graphs and Healpix sampling for cosmological applications}
%\title{HealpixNet: Graph Convolutional Neural Network for efficient spherical convolutions on Healpix sampling for cosmological applications}
% Graph Convolutional Networks for efficient spherical ???
% Efficient spherical ??? with GCNs and Healpix sampling
% (Tomek) How about HealpixCNN? Or HealpixNet? Or HealNet?}
% spherical convolutions with graphs

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{}

\address{}

\begin{abstract}

%Convolutional Neural Networks (CNNs) are becoming an important analysis tool in cosmology and astrophysics.
% Michaël: useful info? To me this is not an argument for our approach. Maybe we have to expand it?

% Architectures are then designed similarly to the classical CNNs used for image analysis.
% HealpixCNN (???) uses Tensorflow as the underlying engine and can utilize most of it functionalities.
% Michaël: not relevant for the abstract in my opinion

Convolutional Neural Networks (CNNs) are now a cornerstone of the Deep Learning toolbox and have led to many breakthroughs in Artificial Intelligence. Despite their successes, they still are mostly developed and applied to regular Euclidean domains such as those supporting images, audio, or video. For many applications however, the data domain is not regular.
As a response to that fact, we are witnessing an increasing interest in the generalization of CNNs to graphs. Graphs are versatile data structures which can represent pairwise relationships between objects or act as a discrete representation of a continuous manifold.
In this paper we take the latter view and represent a sampled sphere as a graph, for which we define the convolution and pooling operations.
Spherical maps appear in many applications, such as the study of phenomena on the Earth for meteorologists or on the sky for cosmologists.
A popular format for spherical maps is Healpix, which uses an equal-area, isolatitude sampling. Hence, we present a spherical CNN for analysis of full and partial Healpix maps, which we call HealpixNet.
While convolution on arbitrary graphs has mostly been sorted out, a general pooling operation has not been discovered so far. As such, while HealpixNet is a special case of a graph CNN, it is tailored to the Healpix sampling of the sphere.
This approach is computationally much more efficient than using spherical harmonics to perform convolutions, an alternative that as been explored previously.
% The filters learned by the HealpixNet are restricted to being radial.
% Michaël: not necessary in the abstract?
We demonstrate the method on a classification problem of mass maps from two cosmological models. A standard benchmark for that problem is to train an SVM on the histograms of map pixel values. Another is to use the power spectral density instead.
HealpixNet achieves \todo{xx\%} better accuracy than those baselines, is more robust to noise, and is \todo{faster?}.
Code and examples are available at \todo{\url{https://github.com/xxx}}.
\end{abstract}

\begin{keyword}
Spherical Convolutional Neural Network \sep SCNN \sep Healpix CNN \sep Graph CNN 
\end{keyword}

\end{frontmatter}

%% \linenumbers

\section{TODO list}
\begin{itemize}
	\item (yes) Michael, do you agree with the title HealpixNet? I (Michaël) extensively modified the abstract to reflect that it is a special case of graph CNNs tailored to Healpix. Hence I now agree with the title. Feel free to edit. Ideally I'd like to have "graph" in the title (graph CNNs are quite hot these days), but I couldn't find how.
    \item Emphasize the novelty of using it on Helapix in abstract, into and rest of the paper
    \item Finish section on Convolutions on graphs (Michael - Nathanael)
    \item Plot of the new filters from Mass maps (Michael)
    \item (done!, plot to be added) check the power spectrum after smoothing
    \item (done!) upgrade the plot of pooling (Nathanael)
    \item new plot for network structure (teaser plot) (Michael)
    \item add info on comparison with histogram, so that it's just a demonstration
    \item (done!) Figure 1 to be round
    \item finish section on Efficient convolutions (Michael)
    \item conclusions
    \item (done!) new simulations with 3 arcmin smoothing
    \item (done!) simulation with cross-validation for the histogram
    \item (done!) Update result section according to the new simulations...
    \item (done!) simulation PSD
    \item (done!) Results figure, add sigma8
    \item (more or less done!) possibly move the eigen decomposition and heat into appendix + add discussion (Nathanael I am not sure about the heat diffusion example...)
\end{itemize}

\section{Introduction}
\label{sec:intro}

%\subsection{Motivation}

Cosmological and astrophysical data often comes in the form of spherical sky maps.
Observables that cover large parts of the sky, such as the Cosmic Microwave Background (CMB) \citep{planck2015cosmologicalparameters,komatsu2011sevenyear,staggs2018recentdiscoveries}, neutral hydrogen \citep{santos2015cosmologySKA,HI4PI2016fullskyHI}, galaxy clustering \citep{alam2017clusteringgalaxies}, gravitational lensing \citep{troxel2017darkenergy,hildebrandt2017kidscosmological}, and others, have been used to constrain cosmological and astrophysical models. Deep Learning has recently gained popularity as an analysis tool in cosmology \citep{schmelze2017cosmologicalmodel,luciesmith2018machinelearning,gupta2018nongaussianinformation,gillet2018deeplearning,hassan2018reionizationmodels,aragoncalvo2018classyfyinglarge,ciuca2017cnnstring}, thanks to its ability to capture complicated, non-linear patterns which are often present in the data. Convolutional Neural Networks (CNNs) are particularly well suited for analysis of astrophysical data due to the translation equivariance built into their architecture. Indeed, sky maps are translation equivariant, which means that rotating maps on the sphere doesn't change their interpretation (at least at the micro scale). As such, learned weights can be shared across the sphere: the network does not have to relearn to detect objects or features at every spatial location.

\todo{@Tomek: can you explain why we care about parts of the sphere? (that is an advantage of the graph model over the spherical harmonics)}
\todo{@Tomek: How is the data acquired? Do telescopes and satellites take measurements according to the Healpix sampling? (it is also an advantage of our method to be agnostic to the sampling, which might be useful to researchers working with missing data or different models)}

So far these algorithms have mostly been demonstrated on Euclidean domains, such as flat images.
As such, perhaps the most straightforward approach to the analysis of spherical data is to divide it into small chunks and project those on flat 2D surfaces.
While this mostly works, as demonstrated \todo{in ??? for example}, it is more natural and accurate to work directly on the sphere.

% cohen2017convolutional (ICML workshop) and cohen2018spherical (ICLR best paper) are redundant
A first attempt to generalize CNNs to spherical data was to apply a standard 2D CNN to a grid discretization of the sphere~\citep{boomsma2017spherical}. While this approach leverages the well-developed 2D convolution and hierarchical pooling, it is restricted to those very specific grid samplings.
A second attempt by~\citet{cohen2018spherical} was to leverage the spherical Fourier transform and to perform the convolution in the spectral domain, thanks to the convolution theorem. While there exists fast Fourier transforms for some samplings, this approach is still computationally expensive.
Finally, the use of a graph to model the discretized sphere was considered by~\citet{khasanova2017graph} as well, towards the goal of building invariances in the network. As~\citep{boomsma2017spherical}, they use an equirectangular grid sampling. Moreover, they did not take advantage of a hierarchical pixelization of the sphere and resorted to dynamic pooling~\citep{kalchbrenner2014dcnn}.

To overcome those shortcomings, we introduce an efficient spherical CNN formulation which uses (i) convolutions on graphs and (ii) hierarchical pooling. The main idea is to represent the sphere $S^2$, a 2-dimensional manifold embedded in $\mathbb{R}^3$, as a graph of connected pixels.
Our approach uses the graph CNN formulation introduced by \citet{defferrard2016convolutional}, which is efficient and can adapt to any sampling of the sphere. The convolution operation costs $\mathcal{O}(n)$ operations, where $n$ is the number of pixels. That is the lowest possible complexity for convolution without approximations (e.g. by sketching).
The flexibility of modeling the data domain with a graph allows, for example, to easily model data which only spans a part of the sphere, or data which is not uniformly sampled.
Our hierarchical pooling strategy exploits a hierarchical pixelization of the manifold to analyze the data at multiple scales. Such hierarchical pixelizations of the sphere have been well studied.

In this work we present and release HealpixNet\footnote{\url{https://github.com/SwissDataScienceCenter/scnn} \todo{change location}}, an algorithm and software package which implements an efficient CNN on Healpix maps. It is implemented with \pkg{TensorFlow} \citep{abadi2016tensorflow} and is intended to be easy to use out-of-the box for cosmological applications.
As HEALPix~\citep{gorski2005healpix} is the most popular sampling used in cosmology and astrophysics, we tailored our method to that particular sampling. However, only two elements depend on the sampling: (i) the choice of neighboring pixels when building the graph, and (ii) the choice of parent pixels when building the hierarchy.
It is also possible to use multiple Healpix maps within our framework and use datasets in form of ``shells''. These shells can span the radial direction, so that a tomographic analysis can be performed, or different frequencies, in case of data from observations in radio frequencies.

%\nati{I am not sure this goes there...} Other recently developed sampling methods include \citep{mcewen2011novelsampling}, where exact sampling theorem for equiangular MW sampling was presented.

We give a practical demonstration of the application of our package to cosmological model discrimination using convergence maps, similar to \citep{schmelze2017cosmologicalmodel}.
In a simplified scenario, we classify convergence maps on parts of a sphere into two cosmological models.
These models were designed to have the same power spectrum in range $\ell < 1000$.
We compare the performance of the spherical CNN to a baseline algorithm, an SVM classifier which takes pixel histograms or Power Spectrum Densities (PSD) of these maps as input.
The comparison is made as a function of noise level on convergence maps and area of the sphere used. Results show that our model is better at discriminating the maps, is more resilient to noise, and \todo{is faster?}.

The paper is organized as follows.
In \secref{related}, we describe existing approaches to convolutions on spheres, graphs, and manifolds.
\secref{method} exposes the construction of a graph given the Healpix sampling of a sphere and defines the convolution operation.
Finally, \secref{experiments} presents our experimental results on the weak lensing mass map classification problem. There we compare our method to two standard benchmarks and show its superiority.
%We conclude in \secref{conclusion}.

% Cite the following papers:
% \begin{itemize}
%     \item Healpix paper \citep{gorski2005healpix}
%     \item First DES mass maps \citep{chang2017curvedsky}
%     \item Healpix convolutions with asymetric beams \citep{mitra2011fastpixel}
%     \item Mass mapping on the sphere \citep{wallis2017mappingdark}
%     \item Wavelets on the sphere \citep{leistedt2016wavelet} and ball \citep{leistedt2012exactwavelets}
%     \item Planck main result \citep{planck2015cosmologicalparameters,komatsu2011sevenyear}
%     \item HI map \citep{HI4PI2016fullskyHI,}
%     \item more radio maps from Adam+Hamsa \citep{santos2015cosmologySKA}
%     \item more astro science on the sphere, what other survey make maps?
%     \item alternative sampling on the sphere \citep{mcewen2011novelsampling} MW sampling
%     \item cite kids tomographic power spectrum \citep{koehlinger2017kidstomographic}
% \end{itemize}


% \subsection{Potential applications}
% 	\assign{Tomek, Nathanaël, Michaël}

% The analysis of spherical cosmological data, such as the cosmic microwave background \cite{...}, as done in \cite{he2018analysis}, is the target application of our method.

% While our method was developed with cosmology in mind, it can easily target any problem where data live on a sphere. Examples include, but are certainly not limited to, (i) efficient compression and decompression of \ang{360} videos (see \cite{su2017learning}), (ii) \todo{data analysis on planets? (climate, forecasting, temperature, wind)}, (iii) \todo{particle physics? (jets on detectors, but they are usually cylindrical)}, (iv) \todo{applications in Cohen's papers?}.

% Finally, not that those neural networks are not restricted to the sphere and can be applied to any problem where we have data on a graph, such as social, biological or infrastructure networks [some citations, e.g. brain Alzeihmer, particle physics, computer graphics].
% the convolution is not restricted to the sphere, the coarsening/pooling is




% About the the different sphere sampling.
% \begin{itemize}
% 	\item Define the spherical grid?
% 	\item reference the "cubed sphere"~\cite{ronchi1996cubed}?
% \end{itemize}


\section{Related work}
\label{sec:related}



\subsection{Spherical convolutions}
\label{sec:spherical_conv}
The major challenge to define Convolution Neural Network (CNN) onto a
spherical domain is to handle a convolution that is suitable for this irregular sampling domain.
Three approaches exist to address this issue.

The first approach, leveraging the continuous spherical convolution, has been
proposed by~\cite{cohen2017convolutional,cohen2018spherical}. The goal of
these contributions it to use a rotational equivariant convolution on the
sphere, i.e. a rotation of the input implies the same rotation of the output.
The resulting convolution is performed by a spherical Fourier transform (i.e.
a projection on the spherical harmonics), a  multiplication in the spectral
domain and an inverse spherical Fourier transform. Hence the main
computational cost of a convolution comes from the two Fourier transforms.
Fortunately, the Fourier transform can be accelerated for special samplings
set (theoretically including HealPix, \cite{mohlenkamp1999fast,rokhlin2006fast}). 
The main advantage of this
approach is that it provides a mathematically well defined rotational
equivariant network. Nevertheless, even with these Fourier transform
acceleration, the convolutions remain expensive in comparison to a traditional
2-dimensional convolution, limiting the practical use of this approach.
\todo{to have numbers to back up our claim: have the theoretical complexities and measure the time needed for a convolution on graph and the healpix Fourier transform to get the constants}
While
a comparison is theoretically possible, the experiments
of~\cite{cohen2018spherical} are done using the geodesic grid sampling set and
do not provide any code for the Healpix one. Hence, a practical comparison is
not possible. We also note that this approach cannot be trivially accelerated
if only one part of the sphere is used. This is impractical in the field of
cosmology as the data often observed only on a subpart of the sphere.
\todo{slow: full sky maps are composed of millions of points. partial maps are hard to process}

A second direction has been followed in~\citet{boomsma2017spherical}. The idea is
to use the traditional 2-dimensional convolution on an irregular grid defined on the
sphere. In the paper, two grids are used: the geodesic grid \todo{isn't it the equirectangular grid?} and the ``cubed
sphere'' grid defined by~\citet{ronchi1996cubed}. Because this
approach is based on the traditional 2-dimensional convolution, it is very likely to be the
most computationally efficient one. However, it suffers that it can only be used using very
specific grid sampling sets which are NOT including HealPix. Furthermore, the
grid requirement makes it impossible for the convolution to capture the
spherical properties of the domain, i.e. the "cubed sphere” sampling is by
definition adapted to a cube and not to a sphere.
\todo{border effects?}
% \nati{Michael: Under some very
% specific hypothesis, this second approach is a particular case of our method,
% i.e. 1) our method with a stupid sampling, 2) assumption that graph convolution
% on a grid == 2d convolution. Do you think we should mention that? I think we
% should not.}
% I think we should!

In an attempt to get the best of the two first approaches, we follow a third
direction. We use a graph to adapt to the particular structure of the sphere.
While the convolution remains efficient, it still captures well the spherical
structure and is particularly adapted to the HealPix sampling.

The use of a graph to model the discretized sphere was considered by~\citet{khasanova2017graph} as well. The authors map omnidirectional images to the sphere, with the goal of correcting for distortions induced by the lens used for the acquisition.
Besides the different application covered by the paper, there are two important
differences. First the sampling is different as~\cite{khasanova2017graph} uses a
grid. Second, it uses a different graph CNN based on a different parametrization
of the convolution kernel.

\todo{Cool to have a global illustration of the network (CNN like)}


\subsection{Convolutions on graphs}
\assign{Michaël}

\todo{other approaches? GNNs, Kipf first order approx, message passing}

The connection has recently been made between graph CNNs and group equivariant CNNs \cite{kondor2018equivariance}, justifying the use of graphs to gain rotation equivariance on the sphere.

Previous work: [Bruna] which needed the full eigendecomposition of the Laplacian, costing $O(n^3)$ operations.

In this work, we are using the graph CNN formulation introduced in~\cite{defferrard2016convolutional}.

Spatial definitions of graph convolutions, e.g. [Niepert] needs to define an orientation in order to match the edges with the filters. Most often the orientation is not given by the application, and one has to define it (for example by ordering by degree or any other measure, or by using a graph coloring). There is no good default good orientation on general graphs and the choice of an orientation is highly application dependent.


\todo{Maybe we just describe convolutions on sphere and graph, i.e: do we really need this two subsections? Maybe simply merge them?}
\subsection{Convolutions on manifolds}


Related to this, convolutional neural networks have been defined on manifolds and have achieved impressive results on shapes [Bronstein]. They however too depend on an orientation, which spheres do not possess.

\subsection{Convolutions on point clouds?}


PointNet and co. Related but we are loosing the structure. Also coarsening.

\section{Method}
\label{sec:method}
% \begin{itemize}
% 	\item We build a graph using the healpix sampling
% 	\item Define Fourier transform and show that the harmonics are visually close to the spherical harmonics
% 	\item Define spherical convolution using the graph Fourier transform and show heat diffusion example
% 	\item Show the limits of the approach and explain why we cannot have a perfect spherical convolution with this technique
% \end{itemize}

The gist of our method is to define the convolution on a sphere using a graph.
The graph is here seen as a discrete approximation of the sphere $S^2$, a
2-dimensional manifold embedded in $\mathbb{R}^3$.

As presented by~\cite{cohen2018spherical}, the most mathematical approach to
extend the convolution on a sphere is to use a spherical Fourier transform. The
convolution is then simply defined as the product in the spectral domain. This
approach requires one Fourier and one inverse Fourier transform per convolution
which remain, even with accelerated algorithms, expensive. For 2-dimensional
images, an efficient convolution can be achieved when the convolution kernel is
localized (for example a 5x5 pixel patch) by doing the computation directly in
the signal domain. Unfortunately, this approach cannot be directly extended to
the spherical case. Hence, the main idea of this contribution is to leverage
graph signal processing~\cite{shuman2013emerging} to define a spherical
convolution that can be computed (and back-propagated) directly in the signal domain.

\subsection{HEALPix sampling}
\label{sec:healpix}

Before doing any numerical analysis on the sphere, one first has to choose a tessellation, i.e., an exhaustive partition of the sphere into finite area elements, where the data under study is quantized.
While our method is applicable to any pixelization of the sphere, two details depend on the chosen sampling: (i) the choice of neighbors in the construction of the graph, and (ii) the choice of parent nodes when coarsening the graph.
As HEALPix~\citep{gorski2005healpix} is the most popular sampling used in cosmology, our target application, we'll tailor the method to that particular sampling in the subsequent exposition.
%We'll note in later sections where the method should be adapted for a different sampling.

HEALPix is a particular case of a more general class of schemes which are based on a hierarchical subdivision of a base polyhedron.
%Many discretized maps of the sphere are based on a hierarchical subdivision of a base polyhedron.
Another example is the geodesic grids which are based on geodesic polyhedrons, i.e., polyhedrons made of triangular faces. A counter-example is the equirectangular projection, which is not constructed from a base polyhedron, although it can be subdivided.
In the particular Healpix case, the base is a rhombic dodecahedron, i.e., a polyhedron made from 12 congruent rhombic faces.\footnote{A rhombus is a quadrilateral whose four sides all have the same length.} See \figref{pooling} for an illustration of the base rhombic dodecahedron and its subdivisions.

HEALPix is an acronym for Hierarchical Equal Area isoLatitude Pixelization of a sphere. As suggested in the name, this pixelization produces a subdivision of a spherical surface in which each pixel covers the same surface area as every other pixel.
The lowest possible resolution is given by the base partitioning of the surface into $N_{pix} = 12$ equal sized pixels (right-most sphere in \figref{pooling}). As each pixel is subdivided in four, the second coarser resolution is $N_{pix} = N_{side}^2 \cdot 12 = 2^2 \cdot 12 = 48$ pixels (middle sphere in \figref{pooling}), the third is $N_{pix} = N_{side}^2 \cdot 12 = 4^2 \cdot 12 = 192$ pixels, etc., where $N_{side} = 1, 2, 4, 8, \ldots$ is the grid resolution parameter.
High-resolutions HEALPix maps easily reach millions of pixels. \figref{example_maps} shows two examples.
Another property of the HEALPix grid is that the pixel centers, represented by the black dots in \figref{pooling}, occur on a discrete number of rings of constant latitude (the number of which depends on the resolution).

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/exampleEarthTopo}
		\caption{}
		\label{fig:example_earth}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/exampleCMB}
		\caption{}
		\label{fig:example_cmb}
	\end{subfigure}
	\caption[]{Example of (a) an Earth topography map composed of \textasciitilde 3 million pixels (\textasciitilde 7 arcmin resolution) and (b) a model of the Cosmic Microwave Background (CMB) radiation temperature anisotropy, composed of \textasciitilde 13 million pixels (\textasciitilde 3.4 arcmin resolution).\footnotemark}
	\label{fig:example_maps}
\end{figure}

\footnotetext{Figures from \url{https://healpix.jpl.nasa.gov/}.}

\subsection{Graph creation}

The graph approximates the 2D manifold that is the surface of the sphere. It can accommodate any sampling, regular or irregular.

In the classical 2-dimenstional convolution, each pixel is connected with the
same weight to its 4 closest neighbors. We use a weighted undirected graph to
generalize this idea to the ubiquitous HealPix
sampling~\citep{gorski2005healpix}.  Each pixel is represented by a node
(vertex) connected to his $8$ or $7$ closest neighbors.\footnote{For some
pixels, the $8^{th}$ nearest neighbor is not well defined.} Given the set of
nearest neighbors, we define the weight matrix $W$ using the following scheme
\begin{equation}
W[i,j]=\begin{cases}
e^{-\frac{\|x_i-x_j\|_2^2}{\sigma^2}} & \text{if pixels $i$ and $j$ are connected, and}\\
0 & \text{otherwise.}\\
\end{cases}
\end{equation}
\todo{why not $1/d$? See Pascal's paper}
Here $x_i$ is a 3-dimensional vector encoding the coordinate of the pixels $i$
on the sphere and $\sigma$ is the mean of $\|x_i-x_j\|_2$ over all connected
pixels $i$ and $j$. This weighting scheme is important as the distances between
pixels varies due to the small sampling irregularities. Some of the
irregularities have been highlighted in the graph displayed in
\figref{healpix_graph_4}.

These small irregularities are important and affect slightly the degree of a
node (or a pixel), defined as $d_i =\sum_j W[i,j]$.

\begin{figure}[!ht]
\centering
% \includegraphics[width=0.45\textwidth]{figures/healpix_graph_4.pdf}
\vspace{-0.5cm}
\includegraphics[width=0.35\textwidth]{figures/half_graph_4.pdf}
\vspace{-0.5cm}
\caption{
Top view of a Healpix graph for half the sphere using an nside parameter of $4$.
Nodes with 7 neighbors are highlighted. \label{fig:healpix_graph_4}
}

\end{figure}

\subsection{Graph Fourier basis and spherical harmonics}
\todo{Add a few extra references}

Because of the irregular sampling, there is no obvious way to define the convolution
directly from the pixel values. Hence we will first make a detour and define a Fourier transform on the
graph.

Following~\cite{shuman2013emerging}, the graph normalized graph Laplacian
defined as $\L = \I - \D^{-1/2} \W \D^{-1/2}$ is a second order differential operator
that can be used to define the graph Fourier basis. Here $D$ is the diagonal
matrix where $\D_{ii}=\b{d}_i$. By construction the Laplacian is symetric positive
semi-definite and hence can be decomposed as $\L=\U \bLambda \U^*$, where $U$ is an
orthonormal matrix of eigenvectors and $\bLambda$, the diagonal matrix of
eigenvalues. The graph Fourier basis is defined as the Laplacian eigenvectors.
The graph Fourier transform of $f$ is simply its projection on $U$ given by
$\hat{\b{f}}=\U^*\b{f}$. Similarly the inverse graph Fourier transform reads $\b{f}=\U\hat{\b{f}}$.
Note that instead of frequencies, the Fourier modes are crescently ordered with
respect of the Laplacian eigenvalues $\bLambda$. In a sense, the Laplacian
eigenvalues correspond to the squared frequencies.

As shown in \figref{graph_harmonics}, it turns out that using the graph
construction described above, the graph Fourier harmonics resemble the
classical spherical harmonics giving us an indication that the graph is able to
capture the spherical properties of the HealPix sampling. This topic is further discusses in Appendix~\ref{sec:spherical_harmonics}.

\subsection{Convolutions on graphs}
\assign{Nathanaël, Michaël} \todo{Add a few extra references}

Graph convolution is defined by generalizing the concept that convolution is a
multiplication in the spectral domain. Given a convolution kernel
$h:\mathbb{R}_+\rightarrow\mathbb{R}$, the convolution of a signal $\b{f}$
is defined as
\begin{equation} \label{eqn:graph_convolution}
h(L)\b{f} = \U h(\bLambda) \U^* \b{f},
\end{equation}
where $h(\bLambda)$ is a diagonal matrix where $h(\bLambda)_{ii}=h(\bLambda_{ii})$.
Equation~\eqnref{graph_convolution} contains three parts: a) $\U^* \b{f}$, the
Fourier transform of $\b{f}$; b) $h(\bLambda)$ the multiplication with the Fourier
kernel $h(\blambda)$; and c) $\U$ the inverse Fourier transform.

The major difference with the classical convolution is that the convolution kernel
$h$ cannot be thought as a $n \times n$ pattern, but is a continuous function
applied to the graph eigenvalues $\bLambda$. For visualization purposes, one can
look at the effect of the convolution on a Kronecker, i.e. one column of the matrix
$h(\L)$. 
Due to the non-regularity of the graph (i.e. the fact
that there is not perfect sampling on the sphere), the convolution effect will slightly
differ from one node to another. In the specific case of the full sphere, these
differences are negligible in most of the cases. However, when considering only subpart of
the sphere, one can observe important border effects.

\paragraph{Example:}
Let us consider the heat diffusion problem
\begin{equation} \label{eqn:heat_equation}
\L \b{f}(t) = \tau \partial_t \b{f}(t),
\end{equation}
where $\b{f}(t): \mathbb{R}_+ \rightarrow \mathbb{R}^N$. Given the initial condition
$\b{f}(0)$, the solution of~\eqnref{heat_equation} can be expressed as
\begin{equation}
\b{f}(t) = e^{-\L \tau t} \b{f}(0) = \U e^{-\bLambda t \tau} \U^* \g{f}(0) = K_t(\L) \b{f}(0),
\end{equation}
which is a convolution of the signal $\b{f}(0)$ with the kernel $K_t(x)=e^{-\tau
t x}$. Here the kernel $K_t$ is applied to the graph eigenvalues $\bLambda$ that
are one generalization of the squared frequencies on the graph. Hence the kernel $K_t$ is also a generalization of the
Gaussian kernel on the sphere. In \figref{gaussian_filters_comparizon} top, we show
the effect of the convolution by diffusing a unit of heat for $\tau=1$ and various
time $t$. In \figref{gaussian_filters_visualization} of
Appendix~\ref{sec:filter_visualization}, we show different visualizations of the
convolution kernel $K_t$. We also compare the graph convolution with the
spherical symmetric Gaussian smoothing
(\figref{gaussian_filters_comparizon} bottom) and observe that both
techniques lead to similar results. While the graph convolution is different
from a true spherical convolution, it can, providing the correct
parameterization, well approximate it.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_sphere.pdf}
\caption{Convolution comparison (Nside=16).
Top: Diffusion of a unit of heat for different times $t$ using the graph.
Bottom: spherical symmetric Gaussian smoothing for different $\sigma$ (arcmin).
Relative difference between graph convolution and spherical smoothing: $10.4$\%, $5.7$\%, $4.8$\%, $3.8$\% }
\label{fig:gaussian_filters_comparizon}
\end{figure*}

This result is another sign that the constructed graph is able to capture the spherical
structure of the HealPix sampling. Furthermore, in some application, where the
exactitude of the convolution is not a requirement, such as de-noising, it means
that we could use the graph convolution instead. Depending on the convolution kernel
this may reveal much more efficient. In the case of a neural network, the fact
that both convolution are not identical is not very relevant since the network
will adapt the convolution at use.


\subsection{Efficient convolutions}
\assign{Michaël}
With the Fourier basis. Because we don't have an FFT, that costs $O(n^3)$ for the eigendecomposition, plus $O(n^2)$ for the transform (to be done for each forward and backward pass).
\begin{itemize}
	\item We need efficient convolution, hence the Chebysheff trick
	\item Explain the different between a polynomial filter convolution and a traditional patch restricted convolution
	\item Define spherical CNN using graph CNN
\end{itemize}

\subsection{Coarsening and Pooling}

Hierarchical pixelization schemes present a natural coarsening. Indeed, each subdivision divides a cell in an equal number of sub-cells. Coarsening is the reverse operation: merging the sub-cells to summarize the data supported on them. Merging cells lead to a coarser graph, hence the name coarsening. Pooling refers to the operation that summarizes the data supported on the merged sub-cells in one child cell. That operation is often taken as the maximum value, but it might be any permutation invariant operation such as the sum or the average. \figref{pooling} illustrates the process.

% The advantage of schemes based on a base polyhedron is that cells are equal-area. While that is not required for the convolution operation when modeling with graphs (it can be corrected by setting edge weights appropriately, see \citet{khasanova2017graph})
% nope: can also do a regular pooling on the equirectangular. It preserves the sampling, and the convolution is adapted to the sampling thanks to the edge weights.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/pooling.pdf}
	\caption{Two levels of coarsening and pooling: groups of 4 cells are merged into one, then the data on them is summarized in one. The coarser cell covers $1/12$ of the sphere.}
	\label{fig:pooling}
\end{figure}


\section{Experiments}
\label{sec:experiments}

% Structure of the section
% 1) Present the goal
% 2) Explain briefly the dataset
% 3) Present the setting
% 4) Describe the result
In this section, we demonstrate the performance of the spherical neural network on a discrimination problem.
Our implementation is available as a small python package \url{https://our.code.link.com}.
Furthermore, all the data to reproduce our experiments are available online.\footnote{\url{https://doi.org/10.5281/zenodo.????} \todo{correct DOI}}.

In our experiment we classify convergence maps, similar to those created with gravitational lensing technique \citep{chang2017curvedsky}.

\subsection{Data}
\label{sec:data}

Convergence maps represent the dimensionless distribution of over- and underdensities of mass in the universe, projected on the sky plane.
The 3D structures are projected using a geometric kernel, value of which depends on the radial distance.
In gravitational lensing, this kernel is dependent on the radial distances between the observer, the mass plane and the plane of source galaxies \citep[see][for review of gravitational lensing]{bartelman2010gravitationallensing}.
The redshifts of the sources was set to $z=1$.

We make full sky N-body simulations for two parameter sets for $\Lambda \rm{CDM} $ cosmological model: model 1 ($\Omega_m=0.31, \sigma_8=0.82$) and model 2 ($\Omega_m=0.26, \sigma_8=0.91$), where $\Omega_m$ is the matter density in the universe and $\sigma_8$ is the normalisation of the matter power spectrum.
Other parameter parameters were set to $h=0.7$ \todo{(MORE PARAMS HERE - CHECK WITH RAPHAEL)}.
These parameters were chosen to have the same spherical harmonic power spectrum for $\ell<1000$.
That means that it is not possible to distinguish between these cosmological models using these maps if the this range of scales is used.
We found that for $\ell>1000$ the differences in power spectrum is $~5\%$.
To remove this information, we additionally smooth the spherical maps with a Gaussian kernel of radius $3$ arcmins. The resulting PSD are displayed in \figref{psd_sigma3}. In the pre-processing step, we also remove the mean of each map.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/psd_sigma3.pdf}
\caption{Power spectrum densities of the noiseless maps. The maps have be smoothed with a Gaussian kernel of radius $3$ arcmins in the preprocessing step.}
\label{fig:psd_sigma3}
\end{figure}

The simulations are created using the fast lightcone method described in~\citep{sgier2018fastgeneration}.
However, we use only a single simulation box, as opposed to two used in that work, as we use source galaxies at lower redshift of $z=1$, instead of $z=1.5$.
As a simulator we use \pkg{L-PICOLA} \citep{howlett2015lpicola}, a fast and approximate code for N-body simulations.
For each class, we generate 30 simulations.

Out of the 60 simulations, 20 are kept for the test set.
\figref{map_sample} shows the full sky simulations and a zoom region for model 1 (top) and model 2 (bottom).
Initial conditions for these simulations were the same, so the differences in structures can only be attributed to different cosmological parameters used to evolve the particle distribution.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.48\textwidth]{figures/smooth_map_class_1.pdf}
\includegraphics[width=0.48\textwidth]{figures/smooth_map_class_2.pdf}
\caption{Example maps from two classes. Top: model 1 ($\Omega_m=0.31, \sigma_8=0.82$). Bottom: model 2 ($\Omega_m=0.26, \sigma_8=0.91$).
The initial conditions for these simulations were the same, so differences arise only due to different cosmological parameters. \nati{Tomek: Is this correct?}}
\label{fig:map_sample}
\end{figure*}



\subsection{Problem formulation}
While the number of maps might sound small for deep learning, the information contained in the maps is very recurrent, allowing us to use parts of the map as sample for our simulation.
Leveraging the properties of the HealPix sampling, we can split each maps
into $12*o^2$ ($o=1,2,4,\dots$) samples that span a smaller part of the sphere. 
As shown in \figref{subpart_sphere}, we used $o=1,2,4$ as the resulting
samples are large enough to suffer from the effects of the spherical geometry. We
decided to only report simulation results for this specific setting as spherical
cosmological data usually does not span the full sphere. Nevertheless, our code
includes an example using the full sphere.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/part_sphere.pdf}
\caption{3 subparts of the sphere with different sizes. Blue: $o=1$. Green: order $o=2$. Yellow: order $o=4$.}
\label{fig:subpart_sphere}
\end{figure}

The goal is build a robust classifier, i.e. an automatic way to
discriminate between the two classes in the presence of noise. 
We check the performance of the classifier as a function of the area used, i.e "the order" $o$ and also as a function of the relative level of the noise.

For all the
experiments, we work with centered Gaussian noise. \todo{Tomek: could you put a
justification.} To improve the
stability of the spherical CNN, we slowly increase the amount of noise added to
the training data over the iterations. Furthermore in order to avoid
overfitting, the noise is randomly produced during the training process. We trained our
model between \nati{$50$ and $200$} epochs depending on the complexity and the size of
the problem. The testing set was simply used to assess the global performance of
the network and to assess if it was overfitting.
Training took less that a day for each model using a Nvidia 1080 TI.

We compare the performance of the spherical CNN to two simple benchmark
algorithms. The two baselines are based on building features with a) the Power
Spectral Densities or b) the histogram of pixels in the maps~\cite{patton2017cosmologicalconstraints}.
After a normalization, those features are then classified use a Support Vector
Machine (SVM) with a linear kernel. Other kernels did not provide better result
while having worse scaling properties. All classifiers are trained with noisy samples. For fairness, we augmented the dataset
of the baseline algorithm in a similar way as for the network, i.e, with
created an "unlimited" number of samples by adding new random realizations of
the noise. We stop adding new samples in the classifier once the validation
error converge to a value. This process is detailled in Appendix~\ref{sec:dataset_augmentation}.

The maps are separated into a training and a testing set. The performance is
assessed by measuring the classification error on a test set. Once the
training maps are sliced into samples, we split them into a training samples
and validation samples. The validation samples is used to tune the
regularization parameter of the SVM using a standard cross-validation
technique.

\subsection{Results}
\assign{Nathanaël, Tomek}

We present in \figref{results} the results for 5 different noise
levels and $3$ different data sizes.  We observe that the PSD features, i.e.
one of the most used technique for model discrimination in cosmology is unable
to perform the desired task. The main reason is that the two different model
have very similar PSDs (see \figref{psd_sigma3}).   As a result, one
need to use other statistics to solve the problem. In this case, we use
histogram features and observe a significant improvement over the PSD.  The
SCNN present the advantage to learn automatically specific features for the
task and also the usual Deep Learning performance bump.

We also note that the histogram features contains information about the
distribution of pixels, but does not include any spatial information. In
contrast, both the PSD features and the SCNN can exploit the spatial
information. Without this scructure, a non scallable fully connected neural network approach would have to be used.



\begin{figure*}[!ht]
\centering
\includegraphics[width=0.32\textwidth]{figures/result_order1.pdf}
\includegraphics[width=0.32\textwidth]{figures/result_order2.pdf}
\includegraphics[width=0.32\textwidth]{figures/result_order4.pdf}
\caption{Classification errors for the 3 different problems.}
\label{fig:results}
\end{figure*}


\subsection{Filter visualization} 
\assign{Nathanaël, Tomek, Michaël} 
Onecommon visualization for CNN is to display the shape of the learned filters.
Since our construction lead to almost (up to the sampling irregularities)
spherical filters, we plot in \figref{learned_filter} both the section (on the
sphere) and a gnomeview projection on a plane of a random selection of learned
filters. The filters are selected from the last layer of the network
corresponding the experiment with order $o=2$ for a relative noise level of
$2$. Nevertheless, all networks presented similar gnomonic and section pattern, at least from a
human perspective. The gnomonic projection clearly exibits the sampling issue
of HealPix. For a detail, on how the convolution kernels are plot, please refer to
Appendix~\ref{sec:filter_visualization}. 
\todo{Tomek: can you say something about the filters.}

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/section_filter_last.pdf} 
\includegraphics[width=0.45\textwidth]{figures/gnonomic_filter_last.pdf}
\caption{24 learned filters selected from the last spherical convolutional layer. Left: section. Right: gnonomic view.}
\label{fig:learned_filter}
\end{figure*}

\section{Discussion}

Because of the spherical domain, the method presented here restricts the shape of the filters learned by the network to be radial.
While this may be a less general design compared to the one used by conventional CNNs on flat images, we found it to still work efficiently for our test cases.
The layered structure of the network can partially compensate for that.
Further generalizations to non-radial filters may be possible [cite?], but we leave it to future work.

\section{Conclusion}
\label{sec:conclusion}
\assign{Nathanaël, Tomek, Michaël}

\section*{Thanks}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Spherical harmonics VS graph Fourier eigenvectors}
\ref{fig:graph_harmonics} displays the first 16 eigenvectors of the
graph Fourier basis used for the convolution. While their is a clear
resemblance with the spherical harmonic, the small irregularities in the
sampling (see \figref{healpix_graph_4}) have an effect on the
graph Fourier eigenvectors. First, conterintuitively, some graph eigenvectors will be
localized~\cite{perraudin2018global}, i.e: they will span only a small subpart
of the sphere.  Second, as Nside increase toward infinity and hence the number
of pixel $n\rightarrow \infty$, we are still unsure if the graph eigenvectors
converge toward the spherical harmonics. We believe that it might be the case
for a different graph construction (fully connected) and that a proof could be
built on top of the work of~\cite{belkin2007convergence}. This is obviously
out of the scope of this contribution.

\label{sec:spherical_harmonics}
\begin{figure*}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{figures/eigenvectors.pdf}
\caption{12 first graph Fourier eigenvectors. Eigenvectors 1-3 could be assotiated to spherical harmonics of order 1, eigenvectors 4-8 to order 2 and 9-15 to order 3. Nevertheless, the graph eigenvectors are only approximating the real spherical harmonics.}
\label{fig:graph_harmonics}
\end{figure*}



\section{Filter visualization}
\label{sec:filter_visualization}

In \figref{gaussian_filters_visualization}, we present three different
visualization for the a healpix graph convolution kernel. The kernel used is defined in the graph spectral domain as $K_t(x)=e^{-\tau t x}$. This is the domain used for our first visualization. In this case, we simply evaluate $K_t$
at the graph eigenvalues $\text{diag}(\bLambda)$. The lower eigenvalues $\lambda$ correspond to the lower frequencies, i.e. the spectral mode with less variation. 
As a second visualization, we can observe the kernel directly on the sphere. To do so, we perform the convolution of a
Kroneker on the sphere plot the result with gnomonic projection. Eventually, as we are
working on the sphere with theoretically spherical filters (up the the irregularities of the sampling), we could only plot the section of the convoluted Kroneker. To do so, we convolve a Kroneker on the equator and use the equator for our section as shown in~\figref{index_section}. Note that, because of the small irregularities in the HealPix
sampling, the second and the third methods are subject to small varation depending on the
chosen Kroneker. 

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_spectral.pdf}
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_gnomonic.pdf}
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_section.pdf}
\caption{3 different visualizations of the convolution kernel $K_t(x)=e^{-\tau t x}$.
Top: graph spectral domain.
Middle: gnomonic projection.
Bottom: section plot.}
\label{fig:gaussian_filters_visualization}
\end{figure*}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/index_plotting_order20_nside16.pdf}
\caption{Indexes selected for the section ploting of \figref{gaussian_filters_visualization} middle. The delta was placed on the yellow node.}
\label{fig:index_section}
\end{figure}

\section{Augmentation of the dataset for the SVM classifier}
\label{sec:dataset_augmentation} The original number of samples $480$, $1920$,
$7680$ respectively for order $o=1,2,4$ is considered rather low for usual
deep learning technique, especially when one desire to build a noise robust
classifier. As a result, we use a classical dataset augmentation technique.
Before entering the network for training each sample is added a random
realization of Gaussian noise. The effect is twofold. First, the network never
ends up seing the same sample and is less likely to overfit the training set.
Second the trained network will be robust to Gaussian noise.

When comparing the network, we want to ensure that the SVM classifier has
access to the same amount of data that the network, i.e, potentially an
infinite number of sample. Hence, we fit different linear SVM classifier using
various amount of training set size until we oberve experimentally that
increasing the amount of data does not improve the classifier. The validation
set remain the same and is used for cross-validation to tune the L2
penalization parameter of SVM. Eventually, we evaluate the final classifier on
the testing set. An example of the evolution of the training and valdication
error is show in \figref{hist_error_evolution}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/hist_error_order4_noise1_5.pdf}
\caption{TODO}
\label{fig:hist_error_evolution}
\end{figure}



%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\section*{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{biblio}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
