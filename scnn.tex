%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01

% \documentclass[preprint,12pt,authoryear]{elsarticle}
\documentclass[final,twocolumn,3p,times,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}


\usepackage[utf8]{inputenc}



%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{color}
\usepackage{url}
\usepackage{hyperref}
\usepackage{graphicx,array}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{aasmacros}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\newcommand{\nati}[1]{{\color[rgb]{.1,.6,.1}{#1}}}

\newcommand{\todo}[1]{{\color[rgb]{.6,.1,.6}{#1}}}

\newcommand{\assign}[1]{{\color[rgb]{.8,.5,.8}{Assigned: #1 }}}


\renewcommand{\b}[1]{{\bm{#1}}}   % bold symbol

% MATH SYMBOLS
\newcommand{\1}{\b{1}}              % all-ones vector
\newcommand{\0}{\b{0}}              % all-zero vector
\newcommand{\g}[1]{\b{#1}}
\renewcommand{\L}{\b{L}} % the laplacian matrix
\newcommand{\W}{\b{W}}
\newcommand{\I}{\b{I}}
\newcommand{\D}{\b{D}}
\newcommand{\U}{\b{U}}
\newcommand{\bLambda}{\b{\Lambda}}
\newcommand{\blambda}{\b{\lambda}}
\newcommand{\pkg}[1]{\texttt{#1}}


\journal{Astronomy and Computing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{HealpixNet: Efficient spherical Convolutional Neural Network with Healpix sampling for cosmological applications}
% \title{HealpixNet: Efficient spherical Convolutional Neural Network with graphs and Healpix sampling for cosmological applications}
%\title{HealpixNet: Graph Convolutional Neural Network for efficient spherical convolutions on Healpix sampling for cosmological applications}
% Graph Convolutional Networks for efficient spherical ???
% Efficient spherical ??? with GCNs and Healpix sampling
% (Tomek) How about HealpixCNN? Or HealpixNet? Or HealNet?}
% spherical convolutions with graphs

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{}

\address{}

\begin{abstract}

%Convolutional Neural Networks (CNNs) are becoming an important analysis tool in cosmology and astrophysics.
% Michaël: useful info? To me this is not an argument for our approach. Maybe we have to expand it?

% Architectures are then designed similarly to the classical CNNs used for image analysis.
% HealpixCNN (???) uses Tensorflow as the underlying engine and can utilize most of it functionalities.
% Michaël: not relevant for the abstract in my opinion

Convolutional Neural Networks (CNNs) are now a cornerstone of the Deep Learning toolbox and have led to many breakthroughs in Artificial Intelligence. Despite their successes, they still are mostly developed and applied to regular Euclidean domains such as those supporting images, audio, or video. For many applications however, the data domain is not regular.
As a response to that fact, we are witnessing an increasing interest in the generalization of CNNs to graphs. Graphs are versatile data structures which can represent pairwise relationships between objects or act as a discrete representation of a continuous manifold.
In this paper we take the latter view and represent a sampled sphere as a graph, for which we define the convolution and pooling operations.
Spherical maps appear in many applications, such as the study of phenomena on the Earth for meteorologists or on the sky for cosmologists.
A popular format for spherical maps is Healpix, which uses an equal-area, isolatitude sampling. Hence, we present a spherical CNN for analysis of full- and partial Healpix maps, which we call HealpixNet.
While convolution on arbitrary graphs has mostly been sorted out, a general pooling operation has not been discovered. As such, while HealpixNet is a special case of a graph CNN, it is tailored to the Healpix sampling of the sphere.
This approach is computationally much more efficient than using spherical harmonics to perform convolutions, an alternative that as been explored.
% The filters learned by the HealpixNet are restricted to being radial.
% Michaël: not necessary in the abstract?
We demonstrate the method on a classification problem of mass maps from two cosmological models. A standard benchmark for that problem is to train an SVM on the histograms of map pixel values. Another is to use the power spectral density instead.
HealpixNet achieves \todo{xx\%} better accuracy than those baselines, is more robust to noise, and is \todo{faster?}.
Code and examples are available at \url{https://github.com/xxx}.
\end{abstract}

\begin{keyword}
Spherical Convolutional Neural Network \sep SCNN \sep Healpix CNN \sep Graph CNN 
\end{keyword}

\end{frontmatter}

%% \linenumbers

\section{TODO list}
\begin{itemize}
	\item (yes) Michael, do you agree with the title HealpixNet? I (Michaël) extensively modified the abstract to reflect that it is a special case of graph CNNs tailored to Healpix. Hence I now agree with the title. Feel free to edit. Ideally I'd like to have "graph" in the title (graph CNNs are quite hot these days), but I couldn't find how.
    \item Emphasize the novelty of using it on Helapix in abstract, into and rest of the paper
    \item Finish section on Convolutions on graphs (Michael - Nathanael)
    \item Plot of the new filters from Mass maps (Michael)
    \item (done!, plot to be added) check the power spectrum after smoothing
    \item (done!) upgrade the plot of pooling (Nathanael)
    \item new plot for network structure (teaser plot) (Michael)
    \item add info on comparison with histogram, so that it's just a demonstration
    \item (done!) Figure 1 to be round
    \item finish section on Efficient convolutions (Michael)
    \item conclusions
    \item (done!) new simulations with 3 arcmin smoothing
    \item (done!) simulation with cross-validation for the histogram
    \item (done!) Update result section according to the new simulations...
    \item (done!) simulation PSD
    \item (done!) Results figure, add sigma8
    \item (more or less done!) possibly move the eigen decomposition and heat into appendix + add discussion (Nathanael I am not sure about the heat diffusion example...)
\end{itemize}

\section{Introduction}
\label{sec:intro}

%\subsection{Motivation}

Cosmological and astrophysical data often comes in a form of spherical sky maps.
Observables that cover large parts of the sky, such as the Cosmic Microwave Background (CMB) \citep{planck2015cosmologicalparameters,komatsu2011sevenyear,staggs2018recentdiscoveries}, neutral hydrogen \citep{santos2015cosmologySKA,HI4PI2016fullskyHI}, galaxy clustering \citep{alam2017clusteringgalaxies}, gravitational lensing \citep{troxel2017darkenergy,hildebrandt2017kidscosmological}, and others, have been used to constrain cosmological and astrophysical models. Deep Learning methods have recently started to gain popularity as an analysis tool in cosmology \citep{schmelze2017cosmologicalmodel,luciesmith2018machinelearning,gupta2018nongaussianinformation,gillet2018deeplearning,hassan2018reionizationmodels,aragoncalvo2018classyfyinglarge,ciuca2017cnnstring}.
Convolutional Neural Networks (CNNs) are particularly well suited for analysis of astrophysical data due to their ability to capture complicated, non-linear patterns, which are often present in the data.

So far these algorithms have been demonstrated on the flat images.
Deep learning-based analysis of larger survey area form requires dividing the data into smaller chunks and projecting them on a flat, 2D surface.
While this process can work efficiently as already demonstrated, it may be more convenient do work with deep learning directly on the sphere.
The most commonly used scheme in astrophysics to divide a sphere into pixels is Healpix \citep{gorski2005healpix}, which uses isolatitude sampling and pixels with the same area.

In this work we present an algorithm and software package \pkg{SCNN-temp-name}\footnote{github.com/SwissDataScienceCenter/scnn} which implements efficient deep convolutional neural networks on Healpix maps.
To achieve this, we represent the sphere $S^2$ as a graph of connected pixels, which forms a 2D manifold embedded in 3D space $\mathbb{R}^3$.
This approach is based on CNN formulations on graphs described by \citet{defferrard2016convolutional} and runs using the \pkg{TensorFlow} \citep{abadi2016tensorflow} engine.
The flexibility of the graph approach allows for faster analysis when data spans only a part of the sphere.
It is also possible to use multiple Healpix maps within our framework and use datasets in form of ``shells''.
These shells can span the radial direction, so that a tomographic analysis can be performed, or different frequencies, in case of data from observations in radio frequencies.

Different formulations of deep learning on the sphere have been proposed in~\citet{cohen2017convolutional,cohen2018spherical,boomsma2017spherical,khasanova2017graph}.
In all of these methods, a different sphere pixelisation scheme is used, making them unsuitable for most cosmological application. 
Hence, our work we focused on the Healpix sampling, as it is the most commonly used in cosmology.
It can be, however, extended to other sampling schemes, by straightforward change in definition of the graph. These approach are discussed in Section~\ref{sec:spherical_conv}.
%\nati{I am not sure this goes there...} Other recently developed sampling methods include \citep{mcewen2011novelsampling}, where exact sampling theorem for equiangular MW sampling was presented.

An important advantage of our method is the speed of its convolution operation as it is of order $\mathcal{O}(n)$, where $n$ is the number of points, which is the lowest possible complexity for convolution without approximations, e.g. sketching.
While our implementation is slower than traditional convolution on a 2 dimensional image, it remain in the same order of magnitude.
MORE HERE

Because of the spherical domain, the method presented here restricts the shape of the filters learned by the network to be radial.
While this may be a less general design compared to the one used by conventional CNNs on flat images, we found it to still work efficiently for our test cases.
The layered structure of the network can partially compensate for that.
Further generalizations to non-radial filters may be possible [cite?], but we leave it to future work.

We give a practical demonstration of the application of our package to cosmological model discrimination using convergence maps, similar to \citep{schmelze2017cosmologicalmodel}.
In a simplified scenario, we classify convergence maps on parts of a sphere into two cosmological models.
These models were designed to have the same power spectrum in range $\ell < 1000$.
We compare the performance of the spherical CNN to a baseline algorithm, an SVM classifier which takes pixel histograms or Power Spectrum Densities (PSD) of these maps as input.
The comparison is made as a function of noise level on convergence maps and area of the sphere used.

The paper is organised as follows.
In Section~\ref{sec:related}, we describe existing approaches to convolutions on the sphere, graphs, and manifolds.
In Section~\ref{sec:method}, we show the way to construct a graph using Healpix sampling and define the convolution operation.
The comparison between the spherical CNN and the benchmark method using the weak lensing mass map classification is presented in Section~\ref{sec:experiments}.
We conclude in Section~\ref{sec:conclusion}.

% Cite the following papers:
% \begin{itemize}
%     \item Healpix paper \citep{gorski2005healpix}
%     \item First DES mass maps \citep{chang2017curvedsky}
%     \item Healpix convolutions with asymetric beams \citep{mitra2011fastpixel}
%     \item Mass mapping on the sphere \citep{wallis2017mappingdark}
%     \item Wavelets on the sphere \citep{leistedt2016wavelet} and ball \citep{leistedt2012exactwavelets}
%     \item Planck main result \citep{planck2015cosmologicalparameters,komatsu2011sevenyear}
%     \item HI map \citep{HI4PI2016fullskyHI,}
%     \item more radio maps from Adam+Hamsa \citep{santos2015cosmologySKA}
%     \item more astro science on the sphere, what other survey make maps?
%     \item alternative sampling on the sphere \citep{mcewen2011novelsampling} MW sampling
%     \item cite kids tomographic power spectrum \citep{koehlinger2017kidstomographic}
% \end{itemize}


% \subsection{Potential applications}
% 	\assign{Tomek, Nathanaël, Michaël}

% The analysis of spherical cosmological data, such as the cosmic microwave background \cite{...}, as done in \cite{he2018analysis}, is the target application of our method.

% While our method was developed with cosmology in mind, it can easily target any problem where data live on a sphere. Examples include, but are certainly not limited to, (i) efficient compression and decompression of \ang{360} videos (see \cite{su2017learning}), (ii) \todo{data analysis on planets? (climate, forecasting, temperature, wind)}, (iii) \todo{particle physics? (jets on detectors, but they are usually cylindrical)}, (iv) \todo{applications in Cohen's papers?}.

% Finally, not that those neural networks are not restricted to the sphere and can be applied to any problem where we have data on a graph, such as social, biological or infrastructure networks [some citations, e.g. brain Alzeihmer, particle physics, computer graphics].
% the convolution is not restricted to the sphere, the coarsening/pooling is




% About the the different sphere sampling.
% \begin{itemize}
% 	\item Define the spherical grid?
% 	\item reference the "cubed sphere"~\cite{ronchi1996cubed}?
% \end{itemize}


\section{Related work}
\label{sec:related}



\subsection{Spherical convolutions}
\label{sec:spherical_conv}
The major challenge to define Convolution Neural Network (CNN) onto a
spherical domain is to handle a convolution that is suitable for this irregular sampling domain.
Three approaches exist to address this issue.

The first approach, leveraging the continuous spherical convolution, has been
proposed by~\cite{cohen2017convolutional,cohen2018spherical}. The goal of
these contributions it to use a rotational equivariant convolution on the
sphere, i.e. a rotation of the input implies the same rotation of the output.
The resulting convolution is performed by a spherical Fourier transform (i.e.
a projection on the spherical harmonics), a  multiplication in the spectral
domain and an inverse spherical Fourier transform. Hence the main
computational cost of a convolution comes from the two Fourier transforms.
Fortunately, the Fourier transform can be accelerated for special samplings
set (theoretically including HealPix, \cite{mohlenkamp1999fast,rokhlin2006fast}). 
The main advantage of this
approach is that it provides a mathematically well defined rotational
equivariant network. Nevertheless, even with these Fourier transform
acceleration, the convolutions remain expensive in comparison to a traditional
2-dimensional convolution, limiting the practical use of this approach. While
a comparison is theoretically possible, the experiments
of~\cite{cohen2018spherical} are done using the geodesic grid sampling set and
do not provide any code for the Healpix one. Hence, a practical comparison is
not possible. We also note that this approach cannot be trivially accelerated
if only one part of the sphere is used. This is impractical in the field of
cosmology as the data often observed only on a subpart of the sphere.

A second direction has been followed in~\cite{boomsma2017spherical}. The idea is
to use the traditional 2-dimensional convolution on an irregular grid defined on the
sphere. In the paper, two grids are used: the geodesic grid and the “cubed
sphere” grid defined by Ronchi et. al~\cite{ronchi1996cubed}. Because this
approach is based on the traditional 2-dimensional convolution, it is very likely to be the
most computationally efficient one. However, it suffers that it can only be used using very
specific grid sampling sets which are NOT including HealPix. Furthermore, the
grid requirement makes it impossible for the convolution to capture the
spherical properties of the domain, i.e. the "cubed sphere” sampling is by
definition adapted to a cube and not to a sphere.
% \nati{Michael: Under some very
% specific hypothesis, this second approach is a particular case of our method,
% i.e. 1) our method with a stupid sampling, 2) assumption that graph convolution
% on a grid == 2d convolution. Do you think we should mention that? I think we
% should not.}

In an attempt to get the best of the two first approaches, we follow a third
direction. We use a graph to adapt to the particular structure of the sphere.
While the convolution remains efficient, it still captures well the spherical
structure and is particularly adapted to the HealPix sampling.

We are not the first one to leverage a graph to define a CNN on a sphere.
\cite{khasanova2017graph} use the same idea to handle omnidirectional images.
Besides the different application covered by the paper, there are two important
differences. First the sampling is different as~\cite{khasanova2017graph} uses a
grid. Second, it uses a different graph CNN based on a different parametrization
of the convolution kernel.

\todo{Cool to have a global illustration of the network (CNN like)}


\subsection{Convolutions on graphs}
\assign{Michaël}

\todo{other approaches? GNNs, Kipf first order approx, message passing}

Previous work: [Bruna] which needed the full eigendecomposition of the Laplacian, costing $O(n^3)$ operations.

In this work, we are using the graph CNN formulation introduced in~\cite{defferrard2016convolutional}.

Spatial definitions of graph convolutions, e.g. [Niepert] needs to define an orientation in order to match the edges with the filters. Most often the orientation is not given by the application, and one has to define it (for example by ordering by degree or any other measure, or by using a graph coloring). There is no good default good orientation on general graphs and the choice of an orientation is highly application dependent.


\todo{Maybe we just describe convolutions on sphere and graph, i.e: do we really need this two subsections? Maybe simply merge them?}
\subsection{Convolutions on manifolds}


Related to this, convolutional neural networks have been defined on manifolds and have achieved impressive results on shapes [Bronstein]. They however too depend on an orientation, which spheres do not possess.

\subsection{Convolutions on point clouds?}


PointNet and co. Related but we are loosing the structure. Also coarsening.

\section{Method}
\label{sec:method}
% \begin{itemize}
% 	\item We build a graph using the healpix sampling
% 	\item Define Fourier transform and show that the harmonics are visually close to the spherical harmonics
% 	\item Define spherical convolution using the graph Fourier transform and show heat diffusion example
% 	\item Show the limits of the approach and explain why we cannot have a perfect spherical convolution with this technique
% \end{itemize}

The gist of our method is to define the convolution on a sphere using a graph.
The graph is here seen as a discrete approximation of the sphere $S^2$, a
2-dimensional manifold embedded in $\mathbb{R}^3$.

As presented by~\cite{cohen2018spherical}, the most mathematical approach to
extend the convolution on a sphere is to use a spherical Fourier transform. The
convolution is then simply defined as the product in the spectral domain. This
approach requires one Fourier and one inverse Fourier transform per convolution
which remain, even with accelerated algorithms, expensive. For 2-dimensional
images, an efficient convolution can be achieved when the convolution kernel is
localized (for example a 5x5 pixel patch) by doing the computation directly in
the signal domain. Unfortunately, this approach cannot be directly extended to
the spherical case. Hence, the main idea of this contribution is to leverage
graph signal processing~\cite{shuman2013emerging} to define a spherical
convolution that can be computed (and back-propagated) directly in the signal domain.

\subsection{Graph creation}
In the classical 2-dimenstional convolution, each pixel is connected with the
same weight to its 4 closest neighbors. We use a weighted undirected graph to
generalize this idea to the ubiquitous HealPix
sampling~\citep{gorski2005healpix}.  Each pixel is represented by a node
(vertex) connected to his $8$ or $7$ closest neighbors.\footnote{For some
pixels, the $8^{th}$ nearest neighbor is not well defined.} Given the set of
nearest neighbors, we define the weight matrix $W$ using the following scheme
\begin{equation}
W[i,j]=\begin{cases}
e^{-\frac{\|x_i-x_j\|_2^2}{\sigma^2}} & \text{if pixels $i$ and $j$ are connected, and}\\
0 & \text{otherwise.}\\
\end{cases}
\end{equation}
Here $x_i$ is a 3-dimensional vector encoding the coordinate of the pixels $i$
on the sphere and $\sigma$ is the mean of $\|x_i-x_j\|_2$ over all connected
pixels $i$ and $j$. This weighting scheme is important as the distances between
pixels varies due to the small sampling irregularities. Some of the
irregularities have been highlighted in the graph displayed in
Figure~\ref{fig:healpix_graph_4}.

These small irregularities are important and affect slightly the degree of a
node (or a pixel), defined as $d_i =\sum_j W[i,j]$.

\begin{figure}[!ht]
\centering
% \includegraphics[width=0.45\textwidth]{figures/healpix_graph_4.pdf}
\vspace{-0.5cm}
\includegraphics[width=0.35\textwidth]{figures/half_graph_4.pdf}
\vspace{-0.5cm}
\caption{
Top view of a Healpix graph for half the sphere using an nside parameter of $4$.
Nodes with 7 neighbors are highlighted. \label{fig:healpix_graph_4}
}

\end{figure}

\subsection{Graph Fourier basis and spherical harmonics}
\todo{Add a few extra references}

Because of the irregular sampling, there is no obvious way to define the convolution
directly from the pixel values. Hence we will first make a detour and define a Fourier transform on the
graph.

Following~\cite{shuman2013emerging}, the graph normalized graph Laplacian
defined as $\L = \I - \D^{-1/2} \W \D^{-1/2}$ is a second order differential operator
that can be used to define the graph Fourier basis. Here $D$ is the diagonal
matrix where $\D_{ii}=\b{d}_i$. By construction the Laplacian is symetric positive
semi-definite and hence can be decomposed as $\L=\U \bLambda \U^*$, where $U$ is an
orthonormal matrix of eigenvectors and $\bLambda$, the diagonal matrix of
eigenvalues. The graph Fourier basis is defined as the Laplacian eigenvectors.
The graph Fourier transform of $f$ is simply its projection on $U$ given by
$\hat{\b{f}}=\U^*\b{f}$. Similarly the inverse graph Fourier transform reads $\b{f}=\U\hat{\b{f}}$.
Note that instead of frequencies, the Fourier modes are crescently ordered with
respect of the Laplacian eigenvalues $\bLambda$. In a sense, the Laplacian
eigenvalues correspond to the squared frequencies.

As shown in Figure~\ref{fig:graph_harmonics}, it turns out that using the graph
construction described above, the graph Fourier harmonics resemble the
classical spherical harmonics giving us an indication that the graph is able to
capture the spherical properties of the HealPix sampling. This topic is further discusses in Appendix~\ref{sec:spherical_harmonics}.

\subsection{Convolutions on graphs}
\assign{Nathanaël, Michaël} \todo{Add a few extra references}

Graph convolution is defined by generalizing the concept that convolution is a
multiplication in the spectral domain. Given a convolution kernel
$h:\mathbb{R}_+\rightarrow\mathbb{R}$, the convolution of a signal $\b{f}$
is defined as
\begin{equation} \label{def:graph_convolution}
h(L)\b{f} = \U h(\bLambda) \U^* \b{f},
\end{equation}
where $h(\bLambda)$ is a diagonal matrix where $h(\bLambda)_{ii}=h(\bLambda_{ii})$.
Equation~\ref{def:graph_convolution} contains three parts: a) $\U^* \b{f}$, the
Fourier transform of $\b{f}$; b) $h(\bLambda)$ the multiplication with the Fourier
kernel $h(\blambda)$; and c) $\U$ the inverse Fourier transform.

The major difference with the classical convolution is that the convolution kernel
$h$ cannot be thought as a $n \times n$ pattern, but is a continuous function
applied to the graph eigenvalues $\bLambda$. For visualization purposes, one can
look at the effect of the convolution on a Kronecker, i.e. one column of the matrix
$h(\L)$. 
Due to the non-regularity of the graph (i.e. the fact
that there is not perfect sampling on the sphere), the convolution effect will slightly
differ from one node to another. In the specific case of the full sphere, these
differences are negligible in most of the cases. However, when considering only subpart of
the sphere, one can observe important border effects.

\paragraph{Example:}
Let us consider the heat diffusion problem
\begin{equation} \label{eq:heat_equation}
\L \b{f}(t) = \tau \partial_t \b{f}(t),
\end{equation}
where $\b{f}(t): \mathbb{R}_+ \rightarrow \mathbb{R}^N$. Given the initial condition
$\b{f}(0)$, the solution of~\ref{eq:heat_equation} can be expressed as
\begin{equation}
\b{f}(t) = e^{-\L \tau t} \b{f}(0) = \U e^{-\bLambda t \tau} \U^* \g{f}(0) = K_t(\L) \b{f}(0),
\end{equation}
which is a convolution of the signal $\b{f}(0)$ with the kernel $K_t(x)=e^{-\tau
t x}$. Here the kernel $K_t$ is applied to the graph eigenvalues $\bLambda$ that
are one generalization of the squared frequencies on the graph. Hence the kernel $K_t$ is also a generalization of the
Gaussian kernel on the sphere. In Figure~\ref{fig:gaussian_filters_comparizon} top, we show
the effect of the convolution by diffusing a unit of heat for $\tau=1$ and various
time $t$. In  Figure~\ref{fig:gaussian_filters_visualization} of
Appendix~\ref{app:filter_visualization}, we show different visualizations of the
convolution kernel $K_t$. We also compare the graph convolution with the
spherical symmetric Gaussian smoothing
(Figure~\ref{fig:gaussian_filters_comparizon} bottom) and observe that both
techniques lead to similar results. While the graph convolution is different
from a true spherical convolution, it can, providing the correct
parameterization, well approximate it.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_sphere.pdf}
\caption{Convolution comparison (Nside=16).
Top: Diffusion of a unit of heat for different times $t$ using the graph.
Bottom: spherical symmetric Gaussian smoothing for different $\sigma$ (arcmin).
Relative difference between graph convolution and spherical smoothing: $10.4$\%, $5.7$\%, $4.8$\%, $3.8$\% }
\label{fig:gaussian_filters_comparizon}
\end{figure*}

This result is another sign that the constructed graph is able to capture the spherical
structure of the HealPix sampling. Furthermore, in some application, where the
exactitude of the convolution is not a requirement, such as de-noising, it means
that we could use the graph convolution instead. Depending on the convolution kernel
this may reveal much more efficient. In the case of a neural network, the fact
that both convolution are not identical is not very relevant since the network
will adapt the convolution at use.


\subsection{Efficient convolutions}
\assign{Michaël}
With the Fourier basis. Because we don't have an FFT, that costs $O(n^3)$ for the eigendecomposition, plus $O(n^2)$ for the transform (to be done for each forward and backward pass).
\begin{itemize}
	\item We need efficient convolution, hence the Chebysheff trick
	\item Explain the different between a polynomial filter convolution and a traditional patch restricted convolution
	\item Define spherical CNN using graph CNN
\end{itemize}


\subsection{Coarsening}
The HealPix sampling presents the advantage to have a natural coarsening method
based on its construction. Each Nside level divides each pixel into 4 subpixels.
We define the pooling process as the inverse operation grouping the 4 subpixels.
Figure~\ref{fig:pooling} illustrates the process.
\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/pooling.pdf}
\caption{Pooling illustration for 2 levels using some part of the mollview projection.
The final patch represents 1/12 of the sphere.}
\label{fig:pooling}
\end{figure}


\section{Experiments}
\label{sec:experiments}

% Structure of the section
% 1) Present the goal
% 2) Explain briefly the dataset
% 3) Present the setting
% 4) Describe the result
In this section, we demonstrate the performance of the spherical neural network on a discrimination problem.
Our implementation is available as a small python package\url{https://our.code.link.com}.
Furthermore, all the data to reproduce our experiments are available online.\footnote{\url{https://doi.org/10.5281/zenodo.????} \todo{correct DOI}}.

In our experiment we classify convergence maps, similar to those created with gravitational lensing technique \citep{chang2017curvedsky}.

\subsection{Data}
\label{sec:data}

Convergence maps represent the dimensionless distribution of over- and underdensities of mass in the universe, projected on the sky plane.
The 3D structures are projected using a geometric kernel, value of which depends on the radial distance.
In gravitational lensing, this kernel is dependent on the radial distances between the observer, the mass plane and the plane of source galaxies \citep[see][for review of gravitational lensing]{bartelman2010gravitationallensing}.
The redshifts of the sources was set to $z=1$.

We make full sky N-body simulations for two parameter set for $\Lambda \rm{CDM} $ cosmological model: model 1 ($\Omega_m=0.31, \sigma_8=0.82$) and model 2 ($\Omega_m=0.26, \sigma_8=0.91$), where $\Omega_m$ is the matter density in the universe and $\sigma_8$ is the normalisation of the matter power spectrum.
Other parameter parameters were set to $h=0.7$ (MORE PARAMS HERE - CHECK WITH RAPHAEL).
These parameters were chosen to have the same spherical harmonic power spectrum for $\ell<1000$.
That means that it is not possible to distinguish between these cosmological models using these maps if the this range of scales is used.
We found that for $\ell>1000$ the differences in power spectrum is $~5\%$.
To remove this information, we additionally smooth the spherical maps with a Gaussian kernel of radius $3$ arcmins. The resulting PSD are displayed in Figure~\ref{fig:psd_sigma3}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/psd_sigma3.pdf}
\caption{Power spectrum densities of the noiseless maps. The maps have be smoothed with a Gaussian kernel of radius $3$ arcmins in the preprocessing step.}
\label{fig:psd_sigma3}
\end{figure}


The simulations are created using the fast lightcone method described in \citep{sgier2018fastgeneration}.
However, we use only a single simulation box, as opposed to two used in that work, as we use source galaxies at lower redshift of $z=1$, instead of $z=1.5$.
As a simulator we use \pkg{L-PICOLA} \citep{howlett2015lpicola}, a fast and approximate code for N-body simulations.
For each class, we generate 30 simulations.
In the experiment we use data on a parts of the sphere, and we check the performance as a function of the area used.
We also test the performance as a function of noise level of the pixels.

As a pre-processing step, we first remove the mean of each sample and we apply a spherical Gaussian smoothing kernel with a radius of $1$ arcmin.
Out of the 60 simulations, 20 are kept for the test set.
% The training and validation set are created later on.
Figure \ref{fig:map_sample} shows the full sky simulations and a zoom region for model 1 (top) and model 2 (bottom).
Initial conditions for these simulations were the same, so the differences in structures can only be attributed to different cosmological parameters used to evolve the particle distribution.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.48\textwidth]{figures/smooth_map_class_1.pdf}
\includegraphics[width=0.48\textwidth]{figures/smooth_map_class_2.pdf}
\caption{Example maps from two classes. Top: model 1 ($\Omega_m=0.31, \sigma_8=0.82$). Bottom: model 2 ($\Omega_m=0.26, \sigma_8=0.91$).
The initial conditions for these simulations were the same, so differences arise only due to different cosmological parameters. \nati{Tomek: Is this correct?}}
\label{fig:map_sample}
\end{figure*}

\subsection{Problem formulation}

Leveraging the properties of the HealPix sampling, we can split each maps
into $12*o^2$ ($o=1,2,4,\dots$) samples that span a smaller part of the sphere. 
The area of the pixels for an Nside of $o$ correspond to the different split of the data.
As shown in Figure~\ref{fig:subpart_sphere}, we used $o=1,2,4$ as the resulting
samples are large enough to suffer from the effects of the spherical geometry. We
decided to only report simulation results for this specific setting as spherical
cosmological data usually does not span the full sphere. Nevertheless, our code
includes an example using the full sphere.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/part_sphere.pdf}
\caption{3 subparts of the sphere with different sizes. Blue: $o=1$. Green: order $o=2$. Yellow: order $o=4$.}
\label{fig:subpart_sphere}
\end{figure}

The goal is build a robust classifier, i.e. an automatic way to
discriminate between the two classes in the presence of noise. For all the
experiments, we work with centered Gaussian noise. \todo{Tomek: could you put a
justification.} All classifiers are trained with noisy samples. To improve the
stability of the spherical CNN, we slowly increase the amount of noise added to
the training data over the iterations. Furthermore in order to avoid
overfitting, the noise is randomly produced during the training process. We trained our
model between $50$ and $200$ epochs depending on the complexity and the size of
the problem. The testing set was simply used to assess the global performance of
the network and to assess if it was overfitting.
Training took less that a day for each model using a Nvidia 1080 TI.

We compare the performance of the spherical CNN to two simple benchmark
algorithms. The two baselines are based on building features with a) the Power
Spectral Densities or b) the histogram of pixels in the maps~\cite{patton2017cosmologicalconstraints}.
After a normalization, those features are then classified use a Support Vector
Machine (SVM) with a linear kernel. Other kernels did not provide better result
while having worse scaling properties. For fairness, we augmented the dataset
of the baseline algorithm in a similar way as for the network, i.e, with
created an "unlimited" number of samples by adding new random realizations of
the noise. We stop adding new samples in the classifier once the validation
error converge to a value. This process is detailled in Appendix~\ref{sec:dataset_augmentation}.

The maps are separated into a training and a testing set. The performance is
assessed by measuring the classification error on a test set. Once the
training maps are sliced into samples, we split them into a training samples
and validation samples. The validation samples is used to tune the
regularization parameter of the SVM using a standard cross-validation
technique\footnote{We also give a small edge to the SVM classifiers by
using cross-validation on the testing set.}.

\subsection{Results}
\assign{Nathanaël, Tomek}

We present in Figure~\ref{fig:results} the results for 5 different noise
levels and $3$ different data sizes.  We observe that the PSD features, i.e.
one of the most used technique for model discrimination in cosmology is unable
to perform the desired task. The main reason is that the two different model
have very similar PSDs (see Figure~\ref{fig:psd_sigma3}).   As a result, one
need to use other statistics to solve the problem. In this case, we use
histogram features and observe a significant improvement over the PSD.  The
SCNN present the advantage to learn automatically specific features for the
task and also the usual Deep Learning performance bump.

We also note that the histogram features contains information about the
distribution of pixels, but does not include any spatial information. In
contrast, both the PSD features and the SCNN can exploit the spatial
information. Without this scructure, a non scallable fully connected neural network approach would have to be used.



\begin{figure*}[!ht]
\centering
\includegraphics[width=0.32\textwidth]{figures/result_order1.pdf}
\includegraphics[width=0.32\textwidth]{figures/result_order2.pdf}
\includegraphics[width=0.32\textwidth]{figures/result_order4.pdf}
\caption{Classification errors for the 3 different problems. \nati{TODO: order is not correct} }
\label{fig:results}
\end{figure*}


\subsection{Interpretation}
\assign{Nathanaël, Tomek, Michaël}

Show the learned filters and feature maps and try to interpret them.
\todo{\\ TK and NP: Figure of filters from the mass map clasification}

Figure: Filters 1D and Gnonomic

\section{Conclusion}
\label{sec:conclusion}
\assign{Nathanaël, Tomek, Michaël}

\section*{Thanks}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

\section{Spherical harmonics VS graph Fourier eigenvectors}
Figure~\ref{fig:graph_harmonics} displays the first 16 eigenvectors of the
graph Fourier basis used for the convolution. While their is a clear
resemblance with the spherical harmonic, the small irregularities in the
sampling (see~\ref{fig:healpix_graph_4}) have two important effects on the
graph Fourier eigenvectors.  First, some graph eigenvectors will be
localized~\cite{perraudin2018global}, i.e: they will span only a small subpart
of the sphere.  Second, as Nside increase toward infinity and hence the number
of pixel $n\rightarrow \infty$, we are still unsure if the graph eigenvectors
converge toward the spherical harmonics. We believe that it might be the case
for a different graph construction (fully connected) and that a proof could be
built on top of the work of~\cite{belkin2007convergence}. This is obviously
out of the scope of this contribution.

\label{sec:spherical_harmonics}
\begin{figure*}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{figures/eigenvectors.pdf}
\caption{12 first graph Fourier eigenvectors.}
\label{fig:graph_harmonics}
\end{figure*}



\section{Filter visualization}
\label{app:filter_visualization}

In Figure~\ref{fig:gaussian_filters_visualization}, we present three different
ways to plot the graph convolution kernel $K_t(x)=e^{-\tau t x}$. First, they can
be plotted in the graph spectral domain. In this case, we simply evaluate $K_t$
at the graph eigenvalues $\text{diag}(\bLambda)$. Second, we can convolve a
delta on the sphere and plot a gnomonic view of the results. Third, since we are
working on the sphere, we can only plot the section of the convoluted delta. In
Figure~\ref{fig:index_section}, we display the selected indexes for the section
plotting. Note that, because of the small irregularities in the HealPix
sampling, the second and the third methods are likely to change depending on the
chosen delta.

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_spectral.pdf}
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_gnomonic.pdf}
\includegraphics[width=0.95\textwidth]{figures/gaussian_filters_section.pdf}
\caption{3 different visualizations of the convolution kernel $K_t(x)=e^{-\tau t x}$.
Top: graph spectral domain.
Middle: gnomonic projection.
Bottom: section plot.}
\label{fig:gaussian_filters_visualization}
\end{figure*}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/index_plotting_order20_nside16.pdf}
\caption{Indexes selected for the section ploting of Figure~\ref{fig:gaussian_filters_visualization} middle. The delta was placed on the yellow node.}
\label{fig:index_section}
\end{figure}

\section{Augementation of the dataset for the SVM classifier}
\label{sec:dataset_augmentation} The original number of samples $480$, $1920$,
$7680$ respectively for order $o=1,2,4$ is considered rather low for usual
deep learning technique, especially when one desire to build a noise robust
classifier. As a result, we use a classical dataset augmentation technique.
Before entering the network for training each sample is added a random
realization of Gaussian noise. The effect is twofold. First, the network never
ends up seing the same sample and is less likely to overfit the training set.
Second the trained network will be robust to Gaussian noise.

When comparing the network, we want to ensure that the SVM classifier has
access to the same amount of data that the network, i.e, potentially an
infinite number of sample. Hence, we fit different linear SVM classifier using
various amount of training set size until we oberve experimentally that
increasing the amount of data does not improve the classifier. The validation
set remain the same and is used for cross-validation to tune the L2
penalization parameter of SVM. Eventually, we evaluate the final classifier on
the testing set. An example of the evolution of the training and valdication
error is show in Figure~\ref{fig:hist_error_evolution}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/hist_error_order4_noise1_5.pdf}
\caption{TODO}
\label{fig:hist_error_evolution}
\end{figure}



%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\section*{Bibliography}
\bibliographystyle{elsarticle-harv}
\bibliography{biblio}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
